{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/driver_monitor/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, os, threading, time, torchvision, json\n",
    "\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from scipy.spatial import distance\n",
    "from pandas import DataFrame\n",
    "from app_utils.accessory_lib import pytorch_system_info\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.io import read_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "facebox_path = os.getcwd() +'/' + 'PIPNet/FaceBoxes_PyTorch/'\n",
    "sys.path.append(facebox_path)\n",
    "\n",
    "from models.faceboxes import FaceBoxes\n",
    "from data import cfg\n",
    "from utils.box_utils import decode\n",
    "from utils.nms_wrapper import nms\n",
    "from layers.functions.prior_box import PriorBox\n",
    "\n",
    "from PIPNet.lib.networks import *\n",
    "from PIPNet.lib.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "initial_time = time.time()\n",
    "elapsed_time = 0\n",
    "ref_frame = 0\n",
    "det_frame = 0\n",
    "fps = 0\n",
    "input_size = 256\n",
    "net_stride = 32\n",
    "num_nb = 10\n",
    "data_name = 'data_300W'\n",
    "experiment_name = 'pip_32_16_60_r101_l2_l1_10_1_nb10'\n",
    "num_lms = 68\n",
    "enable_gaze = True\n",
    "enable_log = False\n",
    "image_scale = 0.0\n",
    "offset_height = 0\n",
    "offset_width = 0\n",
    "det_box_scale = 1.2\n",
    "eye_det = 0.15\n",
    "cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "device = torch.device(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "img_path  = 'dataset/aihub_drowsy_dataset/Validation/image_semi_restrictions'\n",
    "label_path = 'dataset/aihub_drowsy_dataset/Validation/label_semi_restrictions'\n",
    "\n",
    "img_dir_name_list = os.listdir(img_path)\n",
    "img_dir_name_list.sort()\n",
    "\n",
    "label_dir_name_list = os.listdir(label_path)\n",
    "label_dir_name_list.sort()\n",
    "\n",
    "normal_state_img_dir_list = []\n",
    "normal_state_label_dir_list = []\n",
    "yawn_state_img_dir_list =[]\n",
    "yawn_state_label_dir_list = []\n",
    "drowsy_state_img_dir_list = []\n",
    "drowsy_state_label_dir_list = []\n",
    "\n",
    "\n",
    "for img_dir_name in img_dir_name_list:\n",
    "    if img_dir_name[11:13] == '01':\n",
    "        normal_state_img_dir_list.append(img_path + os.sep + img_dir_name)\n",
    "\n",
    "    elif img_dir_name[11:13] == '03':\n",
    "        yawn_state_img_dir_list.append(img_path + os.sep + img_dir_name)\n",
    "\n",
    "    elif img_dir_name[11:13] == '02':\n",
    "        drowsy_state_img_dir_list.append(img_path + os.sep + img_dir_name)\n",
    "\n",
    "for label_dir_name in label_dir_name_list:\n",
    "    if label_dir_name[11:13] == '01':\n",
    "        normal_state_label_dir_list.append(label_path + os.sep + label_dir_name)\n",
    "\n",
    "    elif label_dir_name[11:13] == '03':\n",
    "        yawn_state_label_dir_list.append(label_path + os.sep + label_dir_name)\n",
    "\n",
    "    elif label_dir_name[11:13] == '02':\n",
    "        drowsy_state_label_dir_list.append(label_path + os.sep + label_dir_name)\n",
    "\n",
    "\n",
    "normal_state_img_path_list = []\n",
    "normal_state_label_path_list = []\n",
    "yawn_state_img_path_list =[]\n",
    "yawn_state_label_path_list = []\n",
    "drowsy_state_img_path_list = []\n",
    "drowsy_state_label_path_list = []\n",
    "\n",
    "\n",
    "for normal_state_img_dir in normal_state_img_dir_list:\n",
    "    normal_state_img_file_name_list = os.listdir(normal_state_img_dir)\n",
    "    normal_state_img_file_name_list.sort()\n",
    "\n",
    "    for normal_state_img_file_name in normal_state_img_file_name_list:\n",
    "        normal_state_img_path_list.append(normal_state_img_dir + os.sep + normal_state_img_file_name)\n",
    "\n",
    "for yawn_state_img_dir in yawn_state_img_dir_list:\n",
    "    yawn_state_img_file_name_list = os.listdir(yawn_state_img_dir)\n",
    "    yawn_state_img_file_name_list.sort()\n",
    "\n",
    "    for yawn_state_img_file_name in yawn_state_img_file_name_list:\n",
    "        yawn_state_img_path_list.append(yawn_state_img_dir + os.sep + yawn_state_img_file_name)\n",
    "\n",
    "for drowsy_state_img_dir in drowsy_state_img_dir_list:\n",
    "    drowsy_state_img_file_name_list = os.listdir(drowsy_state_img_dir)\n",
    "    drowsy_state_img_file_name_list.sort()\n",
    "\n",
    "    for drowsy_state_img_file_name in drowsy_state_img_file_name_list:\n",
    "        drowsy_state_img_path_list.append(drowsy_state_img_dir + os.sep + drowsy_state_img_file_name)\n",
    "\n",
    "\n",
    "for normal_state_label_dir in normal_state_label_dir_list:\n",
    "    normal_state_label_file_name_list = os.listdir(normal_state_label_dir)\n",
    "    normal_state_label_file_name_list.sort()\n",
    "\n",
    "    for normal_state_label_file_name in normal_state_label_file_name_list:\n",
    "        normal_state_label_path_list.append(normal_state_label_dir + os.sep + normal_state_label_file_name)\n",
    "\n",
    "for yawn_state_label_dir in yawn_state_label_dir_list:\n",
    "    yawn_state_label_file_name_list = os.listdir(yawn_state_label_dir)\n",
    "    yawn_state_label_file_name_list.sort()\n",
    "\n",
    "    for yawn_state_label_file_name in yawn_state_label_file_name_list:\n",
    "        yawn_state_label_path_list.append(yawn_state_label_dir + os.sep + yawn_state_label_file_name)\n",
    "\n",
    "for drowsy_state_label_dir in drowsy_state_label_dir_list:\n",
    "    drowsy_state_label_file_name_list = os.listdir(drowsy_state_label_dir)\n",
    "    drowsy_state_label_file_name_list.sort()\n",
    "\n",
    "    for drowsy_state_label_file_name in drowsy_state_label_file_name_list:\n",
    "        drowsy_state_label_path_list.append(drowsy_state_label_dir + os.sep + drowsy_state_label_file_name)\n",
    "\n",
    "pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Driver_State_Dataset(Dataset):\n",
    "    def __init__(self, label_path_list, img_path_list):\n",
    "        self.label_path = label_path_list\n",
    "        self.img_path = img_path_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.label_path[idx]) as f:\n",
    "            label_data = json.load(f)\n",
    "            key_points_raw = (label_data['ObjectInfo']['KeyPoints']['Points'])\n",
    "            key_points = [float(i) for i in key_points_raw]\n",
    "\n",
    "        img = read_image(self.img_path[idx])\n",
    "\n",
    "        return key_points, img.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "normal_state_dataset = Driver_State_Dataset(normal_state_label_path_list, normal_state_img_path_list)\n",
    "normal_state_dataloader = DataLoader(normal_state_dataset, batch_size=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from PIPNet/FaceBoxes_PyTorch/weights/Final_FaceBoxes.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:174\n"
     ]
    }
   ],
   "source": [
    "transformations = transforms.Compose([transforms.Resize(448), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "meanface_indices, reverse_index1, reverse_index2, max_len = get_meanface(os.path.join('PIPNet/data', data_name, 'meanface.txt'), num_nb)\n",
    "resnet101 = models.resnet101(weights='ResNet101_Weights.DEFAULT')\n",
    "landmark_net = Pip_resnet101(resnet101, num_nb=num_nb, num_lms=num_lms, input_size=input_size, net_stride=net_stride)\n",
    "\n",
    "landmark_net = landmark_net.to(device)\n",
    "save_dir = os.path.join('PIPNet/snapshots', data_name, experiment_name)\n",
    "weight_file = os.path.join(save_dir, 'epoch%d.pth' % (60 - 1))\n",
    "state_dict = torch.load(weight_file, map_location=device)\n",
    "landmark_net.load_state_dict(state_dict)\n",
    "landmark_net.eval()\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "preprocess = transforms.Compose([transforms.Resize((input_size, input_size)), transforms.ToTensor(), normalize])\n",
    "\n",
    "\n",
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True\n",
    "\n",
    "\n",
    "def remove_prefix(state_dict, prefix):\n",
    "    print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}\n",
    "\n",
    "\n",
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "net = FaceBoxes(phase='test', size=None, num_classes=2)    # initialize detector\n",
    "net = load_model(net, 'PIPNet/FaceBoxes_PyTorch/weights/Final_FaceBoxes.pth', False)\n",
    "net.eval()\n",
    "net = net.to(device)\n",
    "\n",
    "img = cv2.imread(normal_state_img_path_list[0])\n",
    "\n",
    "frame_width = img.shape[1]\n",
    "frame_height = img.shape[0]\n",
    "\n",
    "frame_width_resize = int((1-image_scale)*frame_width)\n",
    "frame_height_resize = int((1-image_scale)*frame_height)\n",
    "\n",
    "scale = torch.Tensor([frame_width_resize, frame_height_resize, frame_width_resize, frame_height_resize])\n",
    "scale = scale.to(device)\n",
    "\n",
    "det_frame = 0\n",
    "image_border = image_scale / 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:25, 10.13it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_eye_pos = []\n",
    "actual_eye_pos = []\n",
    "\n",
    "for label_path, img_path in tqdm(zip(drowsy_state_label_path_list, drowsy_state_img_path_list)):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_tensor = np.float32(img)\n",
    "    img_tensor -= (104, 117, 123)\n",
    "    img_tensor = img_tensor.transpose(2, 0, 1)\n",
    "    img_tensor = torch.from_numpy(img_tensor).unsqueeze(0)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    loc, conf = net(img_tensor)\n",
    "\n",
    "    priorbox = PriorBox(cfg, image_size=(frame_height_resize, frame_width_resize))\n",
    "    priors = priorbox.forward()\n",
    "    priors = priors.to(device)\n",
    "    prior_data = priors.data\n",
    "    boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    boxes = boxes * scale\n",
    "    boxes = boxes.cpu().numpy()\n",
    "    scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "    inds = np.where(scores > 0.05)[0]\n",
    "    boxes = boxes[inds]\n",
    "    scores = scores[inds]\n",
    "    order = scores.argsort()[::-1][:5000]\n",
    "\n",
    "    boxes = boxes[order]\n",
    "    scores = scores[order]\n",
    "\n",
    "    dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "    keep = nms(dets, 0.3, force_cpu=False)\n",
    "    dets = dets[keep, :]\n",
    "    dets = dets[:750, :]\n",
    "\n",
    "    for i, b in enumerate(dets):\n",
    "        if b[4] > 0.5:\n",
    "            b = list(map(int, b))\n",
    "\n",
    "            det_xmin = b[0]\n",
    "            det_ymin = b[1]\n",
    "            det_xmax = b[2]\n",
    "            det_ymax = b[3]\n",
    "            det_width = det_xmax - det_xmin\n",
    "            det_height = det_ymax - det_ymin\n",
    "\n",
    "            det_xmin -= int(det_width * (det_box_scale - 1) / 2)\n",
    "            det_ymin -= int(det_height * (det_box_scale - 1) / 2)\n",
    "            det_xmax += int(det_width * (det_box_scale - 1) / 2)\n",
    "            det_ymax += int(det_height * (det_box_scale - 1) / 2)\n",
    "\n",
    "            det_xmin = max(det_xmin, 0)\n",
    "            det_ymin = max(det_ymin, 0)\n",
    "            det_xmax = min(det_xmax, frame_width - 1)\n",
    "            det_ymax = min(det_ymax, frame_height - 1)\n",
    "\n",
    "            det_width = det_xmax - det_xmin + 1\n",
    "            det_height = det_ymax - det_ymin + 1\n",
    "            det_crop = img[det_ymin:det_ymax, det_xmin:det_xmax, :]\n",
    "            det_crop = cv2.resize(det_crop, (input_size, input_size))\n",
    "            inputs = Image.fromarray(det_crop[:, :, ::-1].astype('uint8'), 'RGB')\n",
    "            inputs = preprocess(inputs).unsqueeze(0)\n",
    "            inputs = inputs.to(device)\n",
    "            lms_pred_x, lms_pred_y, lms_pred_nb_x, lms_pred_nb_y, outputs_cls, max_cls = forward_pip(landmark_net, inputs, preprocess, input_size, net_stride, num_nb)\n",
    "            lms_pred = torch.cat((lms_pred_x, lms_pred_y), dim=1).flatten()\n",
    "            tmp_nb_x = lms_pred_nb_x[reverse_index1, reverse_index2].view(num_lms, max_len)\n",
    "            tmp_nb_y = lms_pred_nb_y[reverse_index1, reverse_index2].view(num_lms, max_len)\n",
    "            tmp_x = torch.mean(torch.cat((lms_pred_x, tmp_nb_x), dim=1), dim=1).view(-1, 1)\n",
    "            tmp_y = torch.mean(torch.cat((lms_pred_y, tmp_nb_y), dim=1), dim=1).view(-1, 1)\n",
    "\n",
    "            lms_pred_merge = torch.cat((tmp_x, tmp_y), dim=1).flatten()\n",
    "            lms_pred = lms_pred.cpu().numpy()\n",
    "            lms_pred_merge = lms_pred_merge.cpu().numpy()\n",
    "\n",
    "            eye_x = (lms_pred_merge[36 * 2:48 * 2:2] * det_width).astype(np.int32) + det_xmin\n",
    "            eye_y = (lms_pred_merge[(36 * 2) + 1:(48 * 2) + 1:2] * det_height).astype(np.int32) + det_ymin\n",
    "\n",
    "            pred_eye_pos.append(np.concatenate((eye_x, eye_y)))\n",
    "\n",
    "            with open(label_path) as f:\n",
    "                label_data = json.load(f)\n",
    "                key_points_raw = (label_data['ObjectInfo']['KeyPoints']['Points'])\n",
    "                key_points = [float(i) for i in key_points_raw]\n",
    "\n",
    "                actual_eye_x = (key_points[36 * 2:48 * 2:2])\n",
    "                actual_eye_y = (key_points[(36 * 2) + 1:(48 * 2) + 1:2])\n",
    "\n",
    "                actual_eye_pos.append(np.concatenate((actual_eye_x, actual_eye_y)))\n",
    "\n",
    "pred_eye_pos = np.array(pred_eye_pos)\n",
    "actual_eye_pos = np.array(actual_eye_pos)\n",
    "\n",
    "\n",
    "pass"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
